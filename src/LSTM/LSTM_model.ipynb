{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "il2zNS-_eZUo",
        "outputId": "201e3ea7-9291-41af-a075-e183aa7abce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 / 22593\n",
            "100 / 22593\n",
            "200 / 22593\n",
            "300 / 22593\n",
            "400 / 22593\n",
            "500 / 22593\n",
            "600 / 22593\n",
            "700 / 22593\n",
            "800 / 22593\n",
            "900 / 22593\n",
            "1000 / 22593\n",
            "1100 / 22593\n",
            "1200 / 22593\n",
            "1300 / 22593\n",
            "1400 / 22593\n",
            "1500 / 22593\n",
            "1600 / 22593\n",
            "1700 / 22593\n",
            "1800 / 22593\n",
            "1900 / 22593\n",
            "2000 / 22593\n",
            "2100 / 22593\n",
            "2200 / 22593\n",
            "2300 / 22593\n",
            "2400 / 22593\n",
            "2500 / 22593\n",
            "2600 / 22593\n",
            "2700 / 22593\n",
            "2800 / 22593\n",
            "2900 / 22593\n",
            "3000 / 22593\n",
            "3100 / 22593\n",
            "3200 / 22593\n",
            "3300 / 22593\n",
            "3400 / 22593\n",
            "3500 / 22593\n",
            "3600 / 22593\n",
            "3700 / 22593\n",
            "3800 / 22593\n",
            "3900 / 22593\n",
            "4000 / 22593\n",
            "4100 / 22593\n",
            "4200 / 22593\n",
            "4300 / 22593\n",
            "4400 / 22593\n",
            "4500 / 22593\n",
            "4600 / 22593\n",
            "4700 / 22593\n",
            "4800 / 22593\n",
            "4900 / 22593\n",
            "5000 / 22593\n",
            "5100 / 22593\n",
            "5200 / 22593\n",
            "5300 / 22593\n",
            "5400 / 22593\n",
            "5500 / 22593\n",
            "5600 / 22593\n",
            "5700 / 22593\n",
            "5800 / 22593\n",
            "5900 / 22593\n",
            "6000 / 22593\n",
            "6100 / 22593\n",
            "6200 / 22593\n",
            "6300 / 22593\n",
            "6400 / 22593\n",
            "6500 / 22593\n",
            "6600 / 22593\n",
            "6700 / 22593\n",
            "6800 / 22593\n",
            "6900 / 22593\n",
            "7000 / 22593\n",
            "7100 / 22593\n",
            "7200 / 22593\n",
            "7300 / 22593\n",
            "7400 / 22593\n",
            "7500 / 22593\n",
            "7600 / 22593\n",
            "7700 / 22593\n",
            "7800 / 22593\n",
            "7900 / 22593\n",
            "8000 / 22593\n",
            "8100 / 22593\n",
            "8200 / 22593\n",
            "8300 / 22593\n",
            "8400 / 22593\n",
            "8500 / 22593\n",
            "8600 / 22593\n",
            "8700 / 22593\n",
            "8800 / 22593\n",
            "8900 / 22593\n",
            "9000 / 22593\n",
            "9100 / 22593\n",
            "9200 / 22593\n",
            "9300 / 22593\n",
            "9400 / 22593\n",
            "9500 / 22593\n",
            "9600 / 22593\n",
            "9700 / 22593\n",
            "9800 / 22593\n",
            "9900 / 22593\n",
            "10000 / 22593\n",
            "10100 / 22593\n",
            "10200 / 22593\n",
            "10300 / 22593\n",
            "10400 / 22593\n",
            "10500 / 22593\n",
            "10600 / 22593\n",
            "10700 / 22593\n",
            "10800 / 22593\n",
            "10900 / 22593\n",
            "11000 / 22593\n",
            "11100 / 22593\n",
            "11200 / 22593\n",
            "11300 / 22593\n",
            "11400 / 22593\n",
            "11500 / 22593\n",
            "11600 / 22593\n",
            "11700 / 22593\n",
            "11800 / 22593\n",
            "11900 / 22593\n",
            "12000 / 22593\n",
            "12100 / 22593\n",
            "12200 / 22593\n",
            "12300 / 22593\n",
            "12400 / 22593\n",
            "12500 / 22593\n",
            "12600 / 22593\n",
            "12700 / 22593\n",
            "12800 / 22593\n",
            "12900 / 22593\n",
            "13000 / 22593\n",
            "13100 / 22593\n",
            "13200 / 22593\n",
            "13300 / 22593\n",
            "13400 / 22593\n",
            "13500 / 22593\n",
            "13600 / 22593\n",
            "13700 / 22593\n",
            "13800 / 22593\n",
            "13900 / 22593\n",
            "14000 / 22593\n",
            "14100 / 22593\n",
            "14200 / 22593\n",
            "14300 / 22593\n",
            "14400 / 22593\n",
            "14500 / 22593\n",
            "14600 / 22593\n",
            "14700 / 22593\n",
            "14800 / 22593\n",
            "14900 / 22593\n",
            "15000 / 22593\n",
            "15100 / 22593\n",
            "15200 / 22593\n",
            "15300 / 22593\n",
            "15400 / 22593\n",
            "15500 / 22593\n",
            "15600 / 22593\n",
            "15700 / 22593\n",
            "15800 / 22593\n",
            "15900 / 22593\n",
            "16000 / 22593\n",
            "16100 / 22593\n",
            "16200 / 22593\n",
            "16300 / 22593\n",
            "16400 / 22593\n",
            "16500 / 22593\n",
            "16600 / 22593\n",
            "16700 / 22593\n",
            "16800 / 22593\n",
            "16900 / 22593\n",
            "17000 / 22593\n",
            "17100 / 22593\n",
            "17200 / 22593\n",
            "17300 / 22593\n",
            "17400 / 22593\n",
            "17500 / 22593\n",
            "17600 / 22593\n",
            "17700 / 22593\n",
            "17800 / 22593\n",
            "17900 / 22593\n",
            "18000 / 22593\n",
            "18100 / 22593\n",
            "18200 / 22593\n",
            "18300 / 22593\n",
            "18400 / 22593\n",
            "18500 / 22593\n",
            "18600 / 22593\n",
            "18700 / 22593\n",
            "18800 / 22593\n",
            "18900 / 22593\n",
            "19000 / 22593\n",
            "19100 / 22593\n",
            "19200 / 22593\n",
            "19300 / 22593\n",
            "19400 / 22593\n",
            "19500 / 22593\n",
            "19600 / 22593\n",
            "19700 / 22593\n",
            "19800 / 22593\n",
            "19900 / 22593\n",
            "20000 / 22593\n",
            "20100 / 22593\n",
            "20200 / 22593\n",
            "20300 / 22593\n",
            "20400 / 22593\n",
            "20500 / 22593\n",
            "20600 / 22593\n",
            "20700 / 22593\n",
            "20800 / 22593\n",
            "20900 / 22593\n",
            "21000 / 22593\n",
            "21100 / 22593\n",
            "21200 / 22593\n",
            "21300 / 22593\n",
            "21400 / 22593\n",
            "21500 / 22593\n",
            "21600 / 22593\n",
            "21700 / 22593\n",
            "21800 / 22593\n",
            "21900 / 22593\n",
            "22000 / 22593\n",
            "22100 / 22593\n",
            "22200 / 22593\n",
            "22300 / 22593\n",
            "22400 / 22593\n",
            "22500 / 22593\n",
            "0 / 4577\n",
            "100 / 4577\n",
            "200 / 4577\n",
            "300 / 4577\n",
            "400 / 4577\n",
            "500 / 4577\n",
            "600 / 4577\n",
            "700 / 4577\n",
            "800 / 4577\n",
            "900 / 4577\n",
            "1000 / 4577\n",
            "1100 / 4577\n",
            "1200 / 4577\n",
            "1300 / 4577\n",
            "1400 / 4577\n",
            "1500 / 4577\n",
            "1600 / 4577\n",
            "1700 / 4577\n",
            "1800 / 4577\n",
            "1900 / 4577\n",
            "2000 / 4577\n",
            "2100 / 4577\n",
            "2200 / 4577\n",
            "2300 / 4577\n",
            "2400 / 4577\n",
            "2500 / 4577\n",
            "2600 / 4577\n",
            "2700 / 4577\n",
            "2800 / 4577\n",
            "2900 / 4577\n",
            "3000 / 4577\n",
            "3100 / 4577\n",
            "3200 / 4577\n",
            "3300 / 4577\n",
            "3400 / 4577\n",
            "3500 / 4577\n",
            "3600 / 4577\n",
            "3700 / 4577\n",
            "3800 / 4577\n",
            "3900 / 4577\n",
            "4000 / 4577\n",
            "4100 / 4577\n",
            "4200 / 4577\n",
            "4300 / 4577\n",
            "4400 / 4577\n",
            "4500 / 4577\n",
            "0 / 8326\n",
            "100 / 8326\n",
            "200 / 8326\n",
            "300 / 8326\n",
            "400 / 8326\n",
            "500 / 8326\n",
            "600 / 8326\n",
            "700 / 8326\n",
            "800 / 8326\n",
            "900 / 8326\n",
            "1000 / 8326\n",
            "1100 / 8326\n",
            "1200 / 8326\n",
            "1300 / 8326\n",
            "1400 / 8326\n",
            "1500 / 8326\n",
            "1600 / 8326\n",
            "1700 / 8326\n",
            "1800 / 8326\n",
            "1900 / 8326\n",
            "2000 / 8326\n",
            "2100 / 8326\n",
            "2200 / 8326\n",
            "2300 / 8326\n",
            "2400 / 8326\n",
            "2500 / 8326\n",
            "2600 / 8326\n",
            "2700 / 8326\n",
            "2800 / 8326\n",
            "2900 / 8326\n",
            "3000 / 8326\n",
            "3100 / 8326\n",
            "3200 / 8326\n",
            "3300 / 8326\n",
            "3400 / 8326\n",
            "3500 / 8326\n",
            "3600 / 8326\n",
            "3700 / 8326\n",
            "3800 / 8326\n",
            "3900 / 8326\n",
            "4000 / 8326\n",
            "4100 / 8326\n",
            "4200 / 8326\n",
            "4300 / 8326\n",
            "4400 / 8326\n",
            "4500 / 8326\n",
            "4600 / 8326\n",
            "4700 / 8326\n",
            "4800 / 8326\n",
            "4900 / 8326\n",
            "5000 / 8326\n",
            "5100 / 8326\n",
            "5200 / 8326\n",
            "5300 / 8326\n",
            "5400 / 8326\n",
            "5500 / 8326\n",
            "5600 / 8326\n",
            "5700 / 8326\n",
            "5800 / 8326\n",
            "5900 / 8326\n",
            "6000 / 8326\n",
            "6100 / 8326\n",
            "6200 / 8326\n",
            "6300 / 8326\n",
            "6400 / 8326\n",
            "6500 / 8326\n",
            "6600 / 8326\n",
            "6700 / 8326\n",
            "6800 / 8326\n",
            "6900 / 8326\n",
            "7000 / 8326\n",
            "7100 / 8326\n",
            "7200 / 8326\n",
            "7300 / 8326\n",
            "7400 / 8326\n",
            "7500 / 8326\n",
            "7600 / 8326\n",
            "7700 / 8326\n",
            "7800 / 8326\n",
            "7900 / 8326\n",
            "8000 / 8326\n",
            "8100 / 8326\n",
            "8200 / 8326\n",
            "8300 / 8326\n",
            "Dataset was prepared\n",
            "Vocabulary was prepared\n",
            "Tokenizer was prepared\n",
            "Dataset was tokenized\n",
            "Model was built\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,936</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ token_and_position_em… │\n",
              "│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            │\n",
              "│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m263,936\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m525,312\u001b[0m │ token_and_position_em… │\n",
              "│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m525,312\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            │\n",
              "│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │          \u001b[38;5;34m2,056\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,316,616</span> (5.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,316,616\u001b[0m (5.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,316,616</span> (5.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,316,616\u001b[0m (5.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'position_embedding' (of type PositionEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "354/354 - 63s - 179ms/step - loss: 1.3604 - perplexity: 3.8485 - val_loss: 1.3403 - val_perplexity: 3.7744\n",
            "Epoch 2/40\n",
            "354/354 - 56s - 159ms/step - loss: 1.3370 - perplexity: 3.7704 - val_loss: 1.3331 - val_perplexity: 3.7597\n",
            "Epoch 3/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.3316 - perplexity: 3.7551 - val_loss: 1.3286 - val_perplexity: 3.7462\n",
            "Epoch 4/40\n",
            "354/354 - 56s - 157ms/step - loss: 1.3266 - perplexity: 3.7381 - val_loss: 1.3241 - val_perplexity: 3.7256\n",
            "Epoch 5/40\n",
            "354/354 - 83s - 233ms/step - loss: 1.3205 - perplexity: 3.7166 - val_loss: 1.3168 - val_perplexity: 3.7072\n",
            "Epoch 6/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.3172 - perplexity: 3.7041 - val_loss: 1.3128 - val_perplexity: 3.6879\n",
            "Epoch 7/40\n",
            "354/354 - 56s - 158ms/step - loss: 1.3082 - perplexity: 3.6719 - val_loss: 1.3312 - val_perplexity: 3.7543\n",
            "Epoch 8/40\n",
            "354/354 - 56s - 158ms/step - loss: 1.3014 - perplexity: 3.6472 - val_loss: 1.3164 - val_perplexity: 3.7008\n",
            "Epoch 9/40\n",
            "354/354 - 142s - 402ms/step - loss: 1.2867 - perplexity: 3.5954 - val_loss: 1.2862 - val_perplexity: 3.5959\n",
            "Epoch 10/40\n",
            "354/354 - 82s - 231ms/step - loss: 1.2766 - perplexity: 3.5602 - val_loss: 1.2804 - val_perplexity: 3.5742\n",
            "Epoch 11/40\n",
            "354/354 - 82s - 231ms/step - loss: 1.2660 - perplexity: 3.5234 - val_loss: 1.2663 - val_perplexity: 3.5253\n",
            "Epoch 12/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.2556 - perplexity: 3.4878 - val_loss: 1.2580 - val_perplexity: 3.4973\n",
            "Epoch 13/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.2458 - perplexity: 3.4546 - val_loss: 1.2480 - val_perplexity: 3.4606\n",
            "Epoch 14/40\n",
            "354/354 - 142s - 401ms/step - loss: 1.2362 - perplexity: 3.4220 - val_loss: 1.2380 - val_perplexity: 3.4295\n",
            "Epoch 15/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.2268 - perplexity: 3.3906 - val_loss: 1.2306 - val_perplexity: 3.4014\n",
            "Epoch 16/40\n",
            "354/354 - 56s - 158ms/step - loss: 1.2184 - perplexity: 3.3627 - val_loss: 1.2186 - val_perplexity: 3.3621\n",
            "Epoch 17/40\n",
            "354/354 - 56s - 157ms/step - loss: 1.2089 - perplexity: 3.3312 - val_loss: 1.2105 - val_perplexity: 3.3343\n",
            "Epoch 18/40\n",
            "354/354 - 56s - 158ms/step - loss: 1.2045 - perplexity: 3.3165 - val_loss: 1.2009 - val_perplexity: 3.3041\n",
            "Epoch 19/40\n",
            "354/354 - 142s - 401ms/step - loss: 1.1938 - perplexity: 3.2817 - val_loss: 1.1955 - val_perplexity: 3.2842\n",
            "Epoch 20/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1853 - perplexity: 3.2543 - val_loss: 1.1852 - val_perplexity: 3.2521\n",
            "Epoch 21/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1780 - perplexity: 3.2309 - val_loss: 1.1775 - val_perplexity: 3.2282\n",
            "Epoch 22/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1715 - perplexity: 3.2100 - val_loss: 1.1715 - val_perplexity: 3.2081\n",
            "Epoch 23/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1646 - perplexity: 3.1883 - val_loss: 1.1659 - val_perplexity: 3.1921\n",
            "Epoch 24/40\n",
            "354/354 - 57s - 160ms/step - loss: 1.1586 - perplexity: 3.1695 - val_loss: 1.1581 - val_perplexity: 3.1691\n",
            "Epoch 25/40\n",
            "354/354 - 56s - 157ms/step - loss: 1.1525 - perplexity: 3.1503 - val_loss: 1.1531 - val_perplexity: 3.1511\n",
            "Epoch 26/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1469 - perplexity: 3.1329 - val_loss: 1.1465 - val_perplexity: 3.1307\n",
            "Epoch 27/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1420 - perplexity: 3.1177 - val_loss: 1.1416 - val_perplexity: 3.1152\n",
            "Epoch 28/40\n",
            "354/354 - 116s - 328ms/step - loss: 1.1365 - perplexity: 3.1008 - val_loss: 1.1373 - val_perplexity: 3.1024\n",
            "Epoch 29/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1315 - perplexity: 3.0854 - val_loss: 1.1295 - val_perplexity: 3.0795\n",
            "Epoch 30/40\n",
            "354/354 - 55s - 156ms/step - loss: 1.1265 - perplexity: 3.0703 - val_loss: 1.1265 - val_perplexity: 3.0686\n",
            "Epoch 31/40\n",
            "354/354 - 143s - 403ms/step - loss: 1.1225 - perplexity: 3.0580 - val_loss: 1.1250 - val_perplexity: 3.0650\n",
            "Epoch 32/40\n",
            "354/354 - 142s - 400ms/step - loss: 1.1205 - perplexity: 3.0522 - val_loss: 1.1178 - val_perplexity: 3.0435\n",
            "Epoch 33/40\n",
            "354/354 - 142s - 401ms/step - loss: 1.1155 - perplexity: 3.0370 - val_loss: 1.1114 - val_perplexity: 3.0238\n",
            "Epoch 34/40\n",
            "354/354 - 142s - 401ms/step - loss: 1.1102 - perplexity: 3.0210 - val_loss: 1.1094 - val_perplexity: 3.0189\n",
            "Epoch 35/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1065 - perplexity: 3.0101 - val_loss: 1.1047 - val_perplexity: 3.0037\n",
            "Epoch 36/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.1025 - perplexity: 2.9982 - val_loss: 1.1001 - val_perplexity: 2.9909\n",
            "Epoch 37/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.0984 - perplexity: 2.9859 - val_loss: 1.0977 - val_perplexity: 2.9838\n",
            "Epoch 38/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.0955 - perplexity: 2.9774 - val_loss: 1.0948 - val_perplexity: 2.9744\n",
            "Epoch 39/40\n",
            "354/354 - 116s - 328ms/step - loss: 1.0930 - perplexity: 2.9702 - val_loss: 1.0897 - val_perplexity: 2.9597\n",
            "Epoch 40/40\n",
            "354/354 - 82s - 232ms/step - loss: 1.0886 - perplexity: 2.9572 - val_loss: 1.0871 - val_perplexity: 2.9510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model was trained\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - loss: 1.0934 - perplexity: 2.9693\n",
            "Validation Set Result: [1.0870920419692993, 2.950958251953125]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'keras_hub.api.utils' has no attribute 'greedy_search'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7ea4731cd285>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-7ea4731cd285>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model was trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/lstm_model_2.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdo_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_TOKENS_TO_GENERATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mWILL_DIRECTLY_EVALUATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inference was made'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7ea4731cd285>\u001b[0m in \u001b[0;36mdo_inference\u001b[0;34m(model, tokenizer, NUM_TOKENS_TO_GENERATE, ds, name)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     output_tokens = keras_nlp.utils.greedy_search(\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mtoken_logits_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras_hub.api.utils' has no attribute 'greedy_search'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "\n",
        "    make_reproducible()\n",
        "    WILL_DIRECTLY_EVALUATE = False\n",
        "    prepare_actual_datasets()\n",
        "    BATCH_SIZE, SEQ_LEN, MIN_TRAINING_SEQ_LEN, EMBED_DIM, LSTM_DIM, NUM_LAYERS, VOCAB_SIZE, EPOCHS, NUM_TOKENS_TO_GENERATE, LR = initialize_hyper_parameters()\n",
        "\n",
        "    raw_train_ds, raw_val_ds, raw_test_ds = prepare_dataset(MIN_TRAINING_SEQ_LEN, BATCH_SIZE)\n",
        "    print('Dataset was prepared')\n",
        "    vocab = get_vocab(raw_train_ds, VOCAB_SIZE)\n",
        "    print('Vocabulary was prepared')\n",
        "    tokenizer = get_tokenizer(vocab, SEQ_LEN)\n",
        "    print('Tokenizer was prepared')\n",
        "    train_ds, val_ds, test_ds = tokenize_dataset(SEQ_LEN, tokenizer, raw_train_ds, raw_val_ds, raw_test_ds)\n",
        "    print('Dataset was tokenized')\n",
        "\n",
        "    if WILL_DIRECTLY_EVALUATE:\n",
        "        print('Model is being read')\n",
        "        model = keras.models.load_model(\"./lstm_model_2.h5\")\n",
        "        print('Model was read')\n",
        "        do_inference(model, tokenizer, NUM_TOKENS_TO_GENERATE, test_ds, 'Test')\n",
        "    else:\n",
        "        model = build_model(VOCAB_SIZE, SEQ_LEN, EMBED_DIM, NUM_LAYERS, LSTM_DIM, LR)\n",
        "        print('Model was built')\n",
        "        history = train_model(model, train_ds, val_ds, EPOCHS)\n",
        "        with open('./trainHistoryDict', 'wb') as file_pi:\n",
        "            pickle.dump(history.history, file_pi)\n",
        "        print('Model was trained')\n",
        "        model.save(\"./lstm_model_2.h5\")\n",
        "        do_inference(model, tokenizer, NUM_TOKENS_TO_GENERATE, val_ds, 'Validation')\n",
        "        WILL_DIRECTLY_EVALUATE = True\n",
        "    print('Inference was made')\n",
        "\n",
        "def initialize_hyper_parameters():\n",
        "    BATCH_SIZE = 64\n",
        "    SEQ_LEN = 2**10 - 1\n",
        "    MIN_TRAINING_SEQ_LEN = 2\n",
        "\n",
        "    EMBED_DIM = 256\n",
        "    LSTM_DIM = 256\n",
        "\n",
        "    NUM_LAYERS = 2\n",
        "    VOCAB_SIZE = 8\n",
        "\n",
        "    EPOCHS = 40\n",
        "    LR = 1e-3\n",
        "\n",
        "    NUM_TOKENS_TO_GENERATE = SEQ_LEN - 1\n",
        "\n",
        "    return BATCH_SIZE, SEQ_LEN, MIN_TRAINING_SEQ_LEN, EMBED_DIM, LSTM_DIM, NUM_LAYERS, VOCAB_SIZE, EPOCHS, NUM_TOKENS_TO_GENERATE, LR\n",
        "\n",
        "def make_reproducible():\n",
        "    seed_value = 364187\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    tf.keras.utils.set_random_seed(seed_value)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "def prepare_actual_datasets():\n",
        "    prepare_actual_dataset('../cleaned_dataset/train.csv', 'train')\n",
        "    prepare_actual_dataset('../cleaned_dataset/validation.csv', 'validation')\n",
        "    prepare_actual_dataset('../cleaned_dataset/test.csv', 'test')\n",
        "\n",
        "def prepare_actual_dataset(path, name):\n",
        "    dataframe = pd.read_csv(path)\n",
        "    gene_nucleotide_sequences = dataframe['NucleotideSequence']\n",
        "    gene_nucleotide_sequences_list = gene_nucleotide_sequences.tolist()\n",
        "\n",
        "    if not os.path.exists('./Datasets'):\n",
        "        os.mkdir('./Datasets')\n",
        "\n",
        "    f = open('./Datasets/' + name + '.txt', 'w')\n",
        "    for i in range(len(gene_nucleotide_sequences_list)):\n",
        "        f.write(' '.join(gene_nucleotide_sequences_list[i][1:-1]))\n",
        "        f.write('\\n\\n')\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(i, '/', len(gene_nucleotide_sequences_list))\n",
        "    f.close()\n",
        "\n",
        "def prepare_dataset(MIN_TRAINING_SEQ_LEN, BATCH_SIZE):\n",
        "    dir = os.path.expanduser(\"./Datasets/\")\n",
        "    raw_train_ds = (\n",
        "        tf.data.TextLineDataset(dir + \"train.txt\")\n",
        "            .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .shuffle(buffer_size=256)\n",
        "    )\n",
        "\n",
        "    raw_val_ds = (\n",
        "        tf.data.TextLineDataset(dir + \"validation.txt\")\n",
        "            .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "            .batch(BATCH_SIZE)\n",
        "    )\n",
        "\n",
        "    raw_test_ds = (\n",
        "        tf.data.TextLineDataset(dir + \"test.txt\")\n",
        "            .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "            .batch(BATCH_SIZE)\n",
        "    )\n",
        "\n",
        "    return raw_train_ds, raw_val_ds, raw_test_ds\n",
        "\n",
        "def get_vocab(raw_train_ds, VOCAB_SIZE):\n",
        "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        raw_train_ds,\n",
        "        vocabulary_size=VOCAB_SIZE,\n",
        "        lowercase=True,\n",
        "        reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        "    )\n",
        "    return vocab\n",
        "\n",
        "def get_tokenizer(vocab, SEQ_LEN):\n",
        "    tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "        vocabulary=vocab,\n",
        "        sequence_length=SEQ_LEN,\n",
        "        lowercase=True,\n",
        "    )\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize_dataset(SEQ_LEN, tokenizer, raw_train_ds, raw_val_ds, raw_test_ds):\n",
        "    start_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=SEQ_LEN,\n",
        "        start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        "    )\n",
        "\n",
        "    def preprocess(inputs):\n",
        "        outputs = tokenizer(inputs)\n",
        "        features = start_packer(outputs)\n",
        "        labels = outputs\n",
        "        return features, labels\n",
        "\n",
        "    train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "        tf.data.AUTOTUNE\n",
        "    )\n",
        "    val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "        tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    test_ds = raw_test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "        tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def build_model(VOCAB_SIZE, SEQ_LEN, EMBED_DIM, NUM_LAYERS, LSTM_DIM, LR):\n",
        "    inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=VOCAB_SIZE,\n",
        "        sequence_length=SEQ_LEN,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        mask_zero=True,\n",
        "    )\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    for _ in range(NUM_LAYERS):\n",
        "        lstm = tf.keras.layers.LSTM(LSTM_DIM, return_sequences=True)\n",
        "        x = lstm(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(optimizer=opt, loss=loss_fn, metrics=[perplexity])\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_ds, val_ds, EPOCHS):\n",
        "    print(model.summary())\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    history = model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS, callbacks=[stop_early])\n",
        "    return history\n",
        "\n",
        "def do_inference(model, tokenizer, NUM_TOKENS_TO_GENERATE, ds, name):\n",
        "    prompt_tokens = tf.convert_to_tensor([tokenizer.token_to_id(\"[BOS]\")])\n",
        "    res = model.evaluate(ds)\n",
        "    print(name, 'Set Result:', res)\n",
        "\n",
        "    def token_logits_fn(inputs):\n",
        "        cur_len = inputs.shape[1]\n",
        "        output = model(inputs)\n",
        "        return output[:, cur_len - 1, :]\n",
        "\n",
        "    output_tokens = keras_nlp.utils.greedy_search(\n",
        "        token_logits_fn,\n",
        "        prompt_tokens,\n",
        "        max_length=NUM_TOKENS_TO_GENERATE,\n",
        "    )\n",
        "    txt = tokenizer.detokenize(output_tokens)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
